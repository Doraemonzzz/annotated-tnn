{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1> The Annotated Tnn </h1></center>\n",
    "\n",
    "\n",
    "<center>\n",
    "<p><a href=\"https://openreview.net/pdf?id=IxmWsm4xrua\">Toeplitz Neural Network for Sequence Modeling</a></p>\n",
    "</center>\n",
    "\n",
    "<img src=\"docs/images/network.png\" width=\"100%\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*博客由[Doreamonzzz](https://github.com/Doraemonzzz)撰写。*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新日志：\n",
    "- 20230313，开始撰写博客；\n",
    "- 20230320，完成动机以及各个部件的实现部分；\n",
    "- 20230524，完成校阅以及引用；"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toeplitz Neural Network(TNN)是一种全新的网络结构，以一种完全不同的方式进行序列建模，在单向/双向语言模型，图像分类任务上和Transformer性能相近，并且在长序列建模[LRA](https://arxiv.org/abs/2011.04006)任务上取得和[S4](https://arxiv.org/abs/2111.00396)相当的性能。这篇博客的主要目的就是以[The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)和[The Annotated S4](https://srush.github.io/annotated-s4/)风格介绍TNN，在阅读完这篇博客后，您将得到如下收获：\n",
    "1. 了解TNN的动机和设计理念；\n",
    "2. 掌握TNN各个部件的实现；\n",
    "<!-- 3. 学习如何将TNN应用在$n$维序列建模任务；\n",
    "4. 了解TNN的优缺点；\n",
    "5. 了解TNN和S4, RWKV等方法的联系； -->\n",
    "\n",
    "总而言之，在阅读完本博客之后，您将成为TNN的专家，并且可以将TNN应用到您的项目中，让我们开始吧。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "- [目录](#目录)\n",
    "- [预备知识](#预备知识)\n",
    "  - [Token mixing and channel mixing](#token-mixing-and-channel-mixing)\n",
    "  - [相对位置编码](#相对位置编码)\n",
    "- [TNN的动机](#tnn的动机)\n",
    "- [TNN的实现](#tnn的实现)\n",
    "  - [准备工作](#准备工作)\n",
    "  - [Tno的实现](#tno的实现)\n",
    "    - [Naive实现](#naive实现)\n",
    "    - [Matrix production实现](#matrix-production实现)\n",
    "    - [FFT实现](#fft实现)\n",
    "    - [Circulant matrix](#circulant-matrix)\n",
    "      - [定义](#定义)\n",
    "      - [快速矩阵乘法](#快速矩阵乘法)\n",
    "      - [实现](#实现)\n",
    "      - [小结](#小结)\n",
    "    - [Toeplitz matrix](#toeplitz-matrix)\n",
    "      - [定义](#定义-1)\n",
    "      - [快速矩阵乘法](#快速矩阵乘法-1)\n",
    "      - [实现](#实现-1)\n",
    "    - [验证实现](#验证实现)\n",
    "    - [补充](#补充)\n",
    "    - [小结](#小结-1)\n",
    "  - [Rpe的实现](#rpe的实现)\n",
    "    - [Naive实现](#naive实现-1)\n",
    "    - [Relative Position Encoder](#relative-position-encoder)\n",
    "    - [实现Relative Position Encoder](#实现relative-position-encoder)\n",
    "    - [将Tno和Rpe合并](#将tno和rpe合并)\n",
    "  - [Tnn layer的实现](#tnn-layer的实现)\n",
    "    - [GLU](#glu)\n",
    "    - [GTU](#gtu)\n",
    "    - [TnnLayer](#tnnlayer)\n",
    "    - [小结](#小结-2)\n",
    "- [全文总结](#全文总结)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预备知识"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token mixing and channel mixing\n",
    "让我们首先从Transformer开始。Transformer作为一个网络结构已经席卷了各个领域，其核心部分主要可以由如下两个计算公式描述：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf X_1 &=\\mathrm{Norm}(\\mathbf X + \\mathrm{MHA}(\\mathbf X)),\\\\\n",
    "\\mathbf O &= \\mathrm{Norm}(\\mathbf X_1 + \\mathrm{FFN}(\\mathbf X_1)).\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中$\\mathbf X \\in \\mathbb R^{n\\times d}$是输入（也可以称为token matrix，其中矩阵的每一行为一个token的向量表示），$n$是序列长度，$d$是特征维度。\n",
    "\n",
    "既然现在有两个主要模块——$\\mathrm {MHA}$和$\\mathrm {FFN}$，那么他们的作用是否有所不同呢？在[Metaformer](https://arxiv.org/abs/2111.11418)一文中，研究者指出，$\\mathrm {MHA}$的主要作用是Token mixing，而$\\mathrm {FFN}$的主要作用是Channel mixing。\n",
    "\n",
    "这是什么意思呢？我们可以从矩阵乘法的角度清晰的理解这点：给定输入（token matrix）$\\mathbf X \\in \\mathbb R^{n\\times d}$，考虑矩阵乘法$\\mathbf A \\mathbf X$和$\\mathbf X \\mathbf B$，那么：\n",
    "- $\\mathbf A \\mathbf X$表示矩阵$\\mathbf X$行的线性组合，而每一行表示一个token，即token的线性组合，所以称为token mixing；\n",
    "- $\\mathbf X  \\mathbf B$表示矩阵$\\mathbf X$列的线性组合，而每一列表示一个channel，即channel的线性组合，所以称为channel mixing；\n",
    "\n",
    "在Transformer中，矩阵$\\mathbf A$即为$\\mathrm{Softmax}(\\mathbf Q \\mathbf K^{\\top} /\\sqrt{d})$，矩阵$\\mathbf B$即为$\\mathrm {FFN}$中的全连接层。\n",
    "\n",
    "大多数对Transformer的改进都是集中在token mixing:$\\mathbf A \\mathbf X$的计算上，以各种各样的方式降低其运算复杂度，TNN也是使用了类似的思路，最核心的一点就是利用了相对位置编码，或者说，Toeplitz矩阵。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相对位置编码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "位置编码是Transformer中的重要组成部分，一开始广为使用的是[绝对位置编码(APE)](https://arxiv.org/abs/1706.03762)，这种编码的方式可以用如下计算方式概括：\n",
    "$$\n",
    "\\mathbf x_i =\\mathbf w_i + \\mathbf p_i.\n",
    "$$\n",
    "其中$\\mathbf w_i$表示第$i$个词的word embedding，$\\mathbf p_i$表示第$i$个位置的position embedding。\n",
    "\n",
    "后来，有研究人员发现，在序列建模中，词的相对位置信息，可能比词的绝位置信息更加重要。\n",
    "> 例如\"我年纪比你大\"的语意和\"我年纪比你大\"完全不同，但是这两句话只是交换了\"你\"和\"我\"的位置。\n",
    ">\n",
    "\n",
    "于是研究人员开始将相对位置编码引入，相对位置编码的使用和绝对位置编码有所不同，其作用在Attention计算的位置：\n",
    "$$\n",
    "\\mathbf s_{ij} = \\mathbf q_i^{\\top} \\mathbf k_j/\\sqrt{d} + t_{i-j}.\n",
    "$$\n",
    "如果写成矩阵的形式则更加直观：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf S & = \\mathbf Q \\mathbf K^{\\top} / \\sqrt {d} + \\mathbf T,\\\\\n",
    "\\mathbf T & =\\left[\\begin{array}{cccc}\n",
    "t_0 & t_{-1} & \\cdots  & t_{-n+1} \\\\\n",
    "t_1 & t_0  &  &  \\vdots \\\\\n",
    "\\vdots &   &   t_0 & t_{-1} \\\\\n",
    "t_{n-1} & \\ldots  & t_1 & t_0\n",
    "\\end{array}\\right] \\in \\mathbb R^{n\\times n}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这里，矩阵$\\mathbf T$有一个数学名称——[Toeplitz矩阵](https://en.wikipedia.org/wiki/Toeplitz_matrix)，不难看出该矩阵有$2n-1$个独立元素。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNN的动机\n",
    "\n",
    "有了之前的准备工作，可以引入我们工作的两个动机：\n",
    "1. 既然相对位置信息如此重要，那么有没有可能只依赖于相对位置信息（Toeplitz matrix）进行token mixing呢？\n",
    "   1. 直观上来说，就是将Attention Matrix替换为Toeplitz matrix。\n",
    "2. 假设(1)成立，那么我们需要进行的主要操作是$\\mathbf T \\mathbf X$，既然矩阵$\\mathbf T$是一个特殊结构的矩阵，那么有没有可能加速运算呢？\n",
    "\n",
    "我们对两个问题都进行了肯定的答复：\n",
    "1. 完全可以只依赖于相对位置信息进行token mixing；\n",
    "2. 由于矩阵的特殊性，可以将运算复杂度由$O(n^2 d)$降低为$O(nd\\log n)$；\n",
    "\n",
    "可以看到，我们的动机极其简单和优雅，最核心的思路就是将$\\mathrm{Softmax}(\\mathbf Q \\mathbf K^{\\top} / \\sqrt {d}) $替换为$\\mathbf T$，但是，这种简单的替换就可以拥有比各种花哨更改更好的性能，这就更加验证了相对位置信息在序列建模中的重要性。\n",
    "\n",
    "\n",
    "<!-- 因此，在后续的讨论快速矩阵乘法时，我们指的是及$\\mathbf T\\mathbf x$，其中$\\mathbf T\\in \\mathbb R^{n\\times n}, \\mathbf x \\in \\mathbb R^{n\\times 1}$。 -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNN的实现\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "接下来的问题就是如何实现TNN，在此之前，我们对之前的公式做一定的调整。\n",
    "\n",
    "在之前的讨论中，我们提到了$\\mathbf T \\mathbf X$可以高效实现，其中$\\mathbf T\\in \\mathbb R^{n\\times n}, \\mathbf X \\in \\mathbb R^{n\\times d}$，这种情况相当于每个channel共享同一个Toeplitz matrix，但是注意到我们可以让不同的channel使用不同的Toeplitz matrix，我们经验上发现，这样一定程度上可以增大模型的表达性，所以在TNN中，**每个channel**使用了不同的Toeplitz matrix。注意到形状为$n\\times n$的Toeplitz matrix实际上只有$2n-1$个独立元素，为了方便后续讨论，我们定义如下映射：$f: \\mathbb R^{(2n-1)\\times 1} \\to \\mathbb R^{n\\times n}$：\n",
    "$$\n",
    "f(\\mathbf t)=f(t_{-n+1},\\ldots, t_{n-1}) =\\left[\\begin{array}{cccc}\n",
    "t_0 & t_{-1} & \\cdots  & t_{-n+1} \\\\\n",
    "t_1 & t_0  &  &  \\vdots \\\\\n",
    "\\vdots &   &   t_0 & t_{-1} \\\\\n",
    "t_{n-1} & \\ldots  & t_1 & t_0\n",
    "\\end{array}\\right] \\in \\mathbb R^{n\\times n}.\n",
    "$$\n",
    "该映射的作用是将维度为$(2n-1)\\times 1$的向量填充为$n\\times n$的Toeplitz matrix。\n",
    "\n",
    "结合之前的记号，我们定义为Tno算子(Toeplitz neural operator)为：\n",
    "$$\n",
    "\\mathrm{Tno}: \\mathbb R^{(2n-1)\\times d}\\times \\mathbb R^{n\\times d} \\to \\mathbb R^{n\\times d},\\\\\n",
    "\\mathbf O= \\mathrm{Tno}(\\mathbf T, \\mathbf X), \\\\\n",
    "\\mathbf O[:, i]= f(\\mathbf T[:, i]) \\mathbf X[:, i].\n",
    "$$\n",
    "\n",
    "备注：这里的记号$\\mathbf T\\in \\mathbb R^{(2n-1)\\times d}$和一开始含义有所不同，注意不要搞混。\n",
    "\n",
    "在开始正式的实现之前，我们先引入一些必要的依赖库以及一些辅助函数：\n",
    "\n",
    "<!-- ，写成计算公式即为：\n",
    "$$\n",
    "\\mathbf O[:, i]= \\mathbf T[:, i] \\mathbf X[:, i], \\\\\n",
    "\\mathbf O[:, i]\\in \\mathbb R^{n\\times 1}, \\mathbf T[:, i]\\in \\mathbb R^{n\\times n},  \\mathbf X[:, i]\\in \\mathbb R^{n\\times 1}.\n",
    "$$ -->\n",
    "\n",
    "<!-- 备注：这里记号有点太严谨，$\\mathbf T[:, i]$的形状应该是$(2n-1) \\times 1$，上述符号是指 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "def get_activation_fn(activation):\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    elif activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"elu\":\n",
    "        return F.elu\n",
    "    elif activation == \"sigmoid\":\n",
    "        return F.sigmoid\n",
    "    elif activation == \"exp\":\n",
    "        return torch.exp\n",
    "    elif activation == \"leak\":\n",
    "        return F.leaky_relu\n",
    "    elif activation == \"1+elu\":\n",
    "        def f(x):\n",
    "            return 1 + F.elu(x)\n",
    "        return f\n",
    "    elif activation == \"2+elu\":\n",
    "            def f(x):\n",
    "                return 2 + F.elu(x)\n",
    "            return f\n",
    "    elif activation == \"silu\":\n",
    "        return F.silu\n",
    "    else:\n",
    "        return lambda x: x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tno的实现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive实现\n",
    "\n",
    "最朴素的实现自然是利用定义进行实现，例如如下代码中，我们使用4重循环，外面两重循环遍历batch, channel维度，第三重循环遍历输出位置，最后一重循环遍历求和项，注意到我们的$\\mathbf T[:, i]$输入形式为$t_{-n+1}, ... , t_{-1}, t_0, t_1, ... , t_{n - 1} $，第三重循环遍历到$i$时，涉及的$t$为$t_{i}, t_{i-1},\\ldots, t_{i-n+1}$，而$n - 1 + i$是$t_{i}$在$\\mathbf T[:, i]$的实际索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tno_naive(x, t):\n",
    "    # x: (b, n, d)\n",
    "    # t: (2n - 1, d), t_(-(n - 1)), ... , t_(-1), t_0, t_1, ... , t_(n - 1) \n",
    "    b, n, d = x.shape\n",
    "    o = torch.zeros_like(x).to(x)\n",
    "    for b_ in range(b):\n",
    "        for d_ in range(d):\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    o[b_][i][d_] += t[n - 1 + i - j][d_] * x[b_][j][d_]\n",
    "\n",
    "    return o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种实现显然太低效，但是至少我们有了一个正确的版本，这对我们后续改进算法也是有帮助的，不难看出这样计算的时间复杂度为$O(n^2d)$，空间复杂度为$O(nd)$（忽略batch维度）。                             "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix production实现\n",
    "第二种实现是并行版本，其思路就是先构造Toeplitz matrix，然后利用矩阵乘法进行计算。最主要的部分是将映射$f$实现出来，代码基于[此处](https://stackoverflow.com/questions/69809789/is-there-any-way-to-create-a-tensor-with-a-specific-pattern-in-pytorch)，主要思路是先将输入改写为$t_0, t_{-1}, ... , t_{1-n}, t_{n - 1}, ... , t_1$，然后构造index $0, 1, \\ldots,n -1, -(n - 1), ..., -1$，将输入映射到Toeplitz matrix，最后得到Toeplitz matrix进行矩阵乘法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tno_matrix(x, t):\n",
    "    # x: (b, n, d)\n",
    "    # t: (2n - 1, d), t_(-(n - 1)), ... , t_(-1), t_0, t_1, ... , t_(n - 1) \n",
    "    n = x.shape[1]\n",
    "    t = t.unsqueeze(0)\n",
    "    # c: t_0, t_1, ... , t_(n - 1)\n",
    "    c = t[:, n - 1:]\n",
    "    # r: t_0, t_(-1), ... , t_(-(n - 1))\n",
    "    r = t[:, :n].flip(1)\n",
    "    # vals: [t_0, t_(-1), ... , t_(-(n - 1)), t_(n - 1), ... , t_1]\n",
    "    vals = torch.cat([r, c[:, 1:].flip(1)], dim=-2)\n",
    "    i, j = torch.ones(n, n).nonzero().T\n",
    "    t_matrix = vals[:, j - i].reshape(n, n, -1)\n",
    "    o = torch.einsum(\"n m d, b m d -> b n d\", t_matrix, x)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种实现的好处是可以利用矩阵乘法，尽管复杂度依然为$O(n^2d)$，但实际效率会快很多；但是由于要构造Toeplitz matrix，所以空间复杂度为$O(n^2d)$，并且这部分还是一个很大的IO开销，所以实际中的速度并不会很快。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT实现\n",
    "有了之前的铺垫，可以看出前两种方法无论是时间复杂度和空间复杂度相比Attention并没有什么优势，那么有没有办法解决这点呢？回答是肯定的，这就需要[FFT]()这把利刃。后续的讨论涉及到一些数学知识，这里先高度概括一下思路：\n",
    "1. 给出Circulant matrix的快速矩阵乘法算法；\n",
    "2. 建立Toeplitz marix和Circulant matrix的关系；"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circulant matrix\n",
    "\n",
    "#### 定义\n",
    "矩阵$\\mathbf C\\in \\mathbb R^{n\\times n}$是一个[Circulant matrix](https://en.wikipedia.org/wiki/Circulant_matrix)当且仅当$\\mathbf C_{ij}= c_{(i-j + n )\\bmod n}$ ,即：\n",
    "$$\n",
    "\\mathbf C=\\left[\\begin{array}{cccccc}\n",
    "c_0 & c_{n-1} &c_{n-2} & \\cdots & \\cdots & c_{1} \\\\\n",
    "c_1 & c_0 & c_{n-1} & \\ddots & & \\vdots \\\\\n",
    "c_2 & c_1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & c_{n-1} & c_{n-2} \\\\\n",
    "\\vdots & & \\ddots & c_1 & c_0 & c_{n-1} \\\\\n",
    "c_{n-1} & \\ldots & \\ldots & c_2 & c_1 & c_0\n",
    "\\end{array}\\right] \\in \\mathbb R^{n\\times n}.\n",
    "$$\n",
    "关于Circulant matrix，有如下重要性质：\n",
    "\n",
    "Circulant matrix $\\mathbf C\\in \\mathbb R^{n\\times n}$正交相似于对角阵$\\mathbf \\Lambda$，特别地，相似矩阵$\\mathbf F$是$n\\times n$ DFT矩阵:\n",
    "$$\n",
    "\\mathbf C = \\mathbf F^{\\top} \\Lambda \\mathbf F, \\\\\n",
    "\\Lambda = \\mathrm{diag}\\{\\mathbf F[c_0,c_1,\\ldots, c_{n-1}]^\\top\\} \\in \\mathbb R^{n\\times n}, \n",
    "{\\mathbf F}_{st}= \\exp\\left(\\frac{2\\pi st i}{n}\\right),i^2=-1.\n",
    "$$\n",
    "证明可以参考[这里](https://ee.stanford.edu/~gray/toeplitz.pdf)。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 快速矩阵乘法\n",
    "\n",
    "现在考虑matrix-vector production操作$\\mathbf M \\mathbf x, \\mathbf M\\in \\mathbb R^{n\\times n}, \\mathbf x\\in \\mathbb R^{n\\times 1}$，那么：\n",
    "- 如果$\\mathbf M$为一般的矩阵，那么该计算的时间复杂度为$O(n^2)$;\n",
    "- 如果$\\mathbf M$为DFT矩阵，那么该计算的时间复杂度为$O(n \\log n)$;\n",
    "\n",
    "\n",
    "基于上述事实，考虑$\\mathbf M=\\mathbf C$为Circulant matrix的情形，那么：\n",
    "$$\n",
    "\\mathbf C \\mathbf x = \\mathbf F^{\\top} \\Lambda \\mathbf F \\mathbf x.\n",
    "$$\n",
    "该计算可以分解为几个步骤：\n",
    "- $\\mathbf x_{\\mathrm{fft}}=\\mathbf{Fx}$；\n",
    "- $\\mathbf c_{\\mathrm{fft}}=\\mathbf F[c_0,c_1,\\ldots, c_{n-1}]^\\top$；\n",
    "- $\\mathbf o_{\\mathrm{fft}}=\\mathbf x_{\\mathrm{fft}}\\odot \\mathbf c_{\\mathrm{fft}}$；\n",
    "- $\\mathbf o= \\mathbf F^{\\top} \\mathbf o_{\\mathrm{fft}}$；\n",
    "\n",
    "其中$\\odot$表示element-wise production，可以看出，算法的总时间复杂度为$O(n\\log n)$，空间复杂度为$O(n)$，所以Circulant matrix对应的矩阵乘法是高效的。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现\n",
    "有了之前的说明，不难利用`fft`实现上述计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circulant_fft(x, c):\n",
    "    # x: (b, n, d)\n",
    "    # c: (n, d), c_0, c_1, ... , c_(n - 1) \n",
    "    n = x.shape[1]\n",
    "    c = c.unsqueeze(0)\n",
    "    x_fft = torch.fft.rfft(x, n, dim=-2)\n",
    "    c_fft = torch.fft.rfft(c, n, dim=-2)\n",
    "    o_fft = x_fft * c_fft\n",
    "    o = torch.fft.irfft(o_fft, n, dim=-2)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经有了一个关于Circulant matrix的高效矩阵乘法，那么下一个问题就是建立Toeplitz matrix和Circulant matrix的关系。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toeplitz matrix\n",
    "#### 定义\n",
    "矩阵$\\mathbf T\\in \\mathbb R^{n\\times n}$是一个Toeplitz matrix当且仅当$\\mathbf T_{ij}= t_{i-j}$，即\n",
    "$$\n",
    "\\mathbf T=\\left[\\begin{array}{cccccc}\n",
    "t_0 & t_{-1} &t_{-2} & \\cdots & \\cdots & t_{-n+1} \\\\\n",
    "t_1 & t_0 & t_{-1} & \\ddots & & \\vdots \\\\\n",
    "t_2 & t_1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & t_{-1} & t_{n-2} \\\\\n",
    "\\vdots & & \\ddots & t_1 & t_0 & t_{-1} \\\\\n",
    "t_{n-1} & \\ldots & \\ldots & t_2 & t_1 & t_0\n",
    "\\end{array}\\right] \\in \\mathbb R^{n\\times n}.\n",
    "$$\n",
    "从形式上来看，Toeplitz matrix和Circulant matrix非常像，唯一的区别在于前者的独立元素数量为$2n-1$，后者的独立元素数量为$n$，那么一个简单的思路就是将Toeplitz matrix嵌入到一个阶数大于等于$2n-1$矩阵中，而这个矩阵本生是一个Circulant matrix，下面来看下这是如何具体操作的。\n",
    "\n",
    "可以将Toeplitz matrix $\\mathbf T\\in \\mathbb R^{n\\times }$嵌入到Circulant matrix $\\mathbf C \\in \\mathbb R^{2n\\times 2n}$中:\n",
    "$$\n",
    "c_{k} =\\begin{cases}\n",
    "t_k , 0 \\le k \\le n - 1\\\\\n",
    "t_0 , k=n\\\\\n",
    "t_{k -2n},   n+1\\le k \\le 2n-1\n",
    "\\end{cases} ,\n",
    "$$\n",
    "即，\n",
    "$$\n",
    "\\mathbf C=\\left[\\begin{array}{ccccc|ccccc}\n",
    "t_0 & t_{-1} & \\ldots & \\ldots & t_{-n+1} & t_0 & t_{n-1} & \\ldots & t_2 & t_1 \\\\\n",
    "t_1 & t_0 & \\ddots & & \\vdots & t_{-n+1} & \\ddots & \\ddots & & t_2 \\\\\n",
    "t_2 & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots & \\ddots & & \\ddots & \\vdots \\\\\n",
    "\\vdots & & \\ddots & t_0 & t_{-1} & t_{-2} & & \\ddots & \\ddots & t_{n-1} \\\\\n",
    "t_{n-1} & \\ldots & \\ldots & t_1 & t_0 & t_{-1} & t_{-2} & \\ldots & t_{-n+1} & t_0 \\\\\n",
    "\\hline t_0 & t_{n-1} & \\ldots & \\ldots & t_1 & t_0 & t_{-1} & \\ldots & \\ldots & t_{-n+1} \\\\\n",
    "t_{-n+1} & \\ddots & \\ddots & & t_2 & t_1 & t_0 & \\ddots & & \\vdots \\\\\n",
    "\\vdots & \\ddots & & \\ddots & \\vdots & t_2 & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "t_{-2} & & \\ddots & \\ddots & t_{n-1} & \\vdots & & \\ddots & t_0 & t_{-1} \\\\\n",
    "t_{-1} & t_{-2} & \\ldots & \\ldots & t_0 & t_{n-1} & \\ldots & \\ldots & t_1 & t_0\n",
    "\\end{array}\\right] \\in \\mathbb R^{2n\\times 2n}.\n",
    "$$\n",
    "\n",
    "使用分块矩阵的符号，我们可以定义：\n",
    "$$\n",
    "\\begin{gathered}\n",
    " \\mathbf C = \\left[\\begin{array}{cc}\n",
    "\\mathbf C_1 & \\mathbf C_2\\\\\n",
    "\\mathbf C_3 & \\mathbf C_4\\\\\n",
    "\\end{array}\\right] \\in \\mathbb R^{2n\\times 2n},\\mathbf C_s \\in \\mathbb R^{n \\times n}, s=1,2,3,4,\n",
    "\\mathbf C_1 = \\mathbf T \n",
    "\\end{gathered}.\n",
    "$$\n",
    "\n",
    "有了上述准备工作，可以得到Toeplitz matrix-vector production的快速算法。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 快速矩阵乘法\n",
    "\n",
    "对于向量$\\mathbf x\\in \\mathbb R^{n}$, 定义:\n",
    "$$\n",
    "\\mathbf x_1 = \\left[\\begin{array}{c}\n",
    "\\mathbf x\\\\\n",
    "\\mathbf 0_n\n",
    "\\end{array}\\right] \\in \\mathbb R^{2n},\n",
    "$$\n",
    "所以，\n",
    "$$\n",
    "\\mathbf C \\mathbf x_1 =\\left[\\begin{array}{cc}\n",
    "\\mathbf C_1 & \\mathbf C_2\\\\\n",
    "\\mathbf C_3 & \\mathbf C_4\\\\\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "\\mathbf x\\\\\n",
    "\\mathbf 0_n\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\mathbf C_1 \\mathbf x\\\\\n",
    "\\mathbf C_3 \\mathbf x\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\mathbf T \\mathbf x\\\\\n",
    "\\mathbf C_3 \\mathbf x\n",
    "\\end{array}\\right] \\in \\mathbb R^{2n},\n",
    "$$\n",
    "因此:\n",
    "$$\n",
    "\\left[\\begin{matrix}\n",
    "{\\mathbf I}_n &\n",
    "{\\mathbf 0}_{n\\times n}\n",
    "\\end{matrix}\\right]\\mathbf C \\mathbf x_1  =\n",
    "\\left[\\begin{matrix}\n",
    "\\mathbf I_n &\n",
    "\\mathbf 0_{n\\times n}\n",
    "\\end{matrix}\\right]\\left[\\begin{array}{c}\n",
    "\\mathbf T \\mathbf x\\\\\n",
    "\\mathbf C_3 \\mathbf x\n",
    "\\end{array}\\right]=\\mathbf T \\mathbf x.\n",
    "$$\n",
    "关于时间复杂度，注意到我们是将$n\\times n$的Toeplitz matrix嵌入到一个$2n\\times 2n$的Circulant matrix中，所以时间复杂度仍然为$O(n\\log n)$。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现\n",
    "和Circulant matrix的情形类似，可以利用`fft`实现上述计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tno_fft(x, t):\n",
    "    # x: (b, n, d)\n",
    "    # t: (2 * n, d), t0, t1, ..., t(n-1), t0, t_(-(n-1)), ... , t_(-1)\n",
    "    n = x.shape[1]\n",
    "    t = t.unsqueeze(0)\n",
    "    x_fft = torch.fft.rfft(x, 2 * n, dim=-2)\n",
    "    t_fft = torch.fft.rfft(t, 2 * n, dim=-2)\n",
    "    o_fft = x_fft * t_fft\n",
    "    o = torch.fft.irfft(o_fft, 2 * n, dim=-2)[:, :n]\n",
    "\n",
    "    return o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证实现\n",
    "在之前的讨论中，我们给出了Tno的三种实现方式，在本节中，我们将验证这些实现的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output error between tno_naive and tno_matrix is 2.414959999441635e-05\n",
      "The output error between tno_naive and tno_matrix is 5.38119456905406e-05\n"
     ]
    }
   ],
   "source": [
    "b = 2\n",
    "n = 16\n",
    "d = 128\n",
    "\n",
    "t_zero = torch.randn(1, d)\n",
    "# t1, ..., t(n-1)\n",
    "t_pos = torch.randn(n - 1, d)\n",
    "# t-(n-1), ... , t-1\n",
    "t_neg = torch.randn(n - 1, d)\n",
    "t1 = torch.cat([t_neg, t_zero, t_pos], dim=0).cuda()\n",
    "t2 = torch.cat([t_zero, t_pos, t_zero, t_neg], dim=0).cuda()\n",
    "x = torch.randn(b, n, d).cuda()\n",
    "\n",
    "o1 = tno_naive(x, t1)\n",
    "o2 = tno_matrix(x, t1)\n",
    "o3 = tno_fft(x, t2)\n",
    "\n",
    "print(f\"The output error between tno_naive and tno_matrix is {torch.norm(o1 - o2)}\")\n",
    "print(f\"The output error between tno_naive and tno_matrix is {torch.norm(o1 - o3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充\n",
    "现在我们已经完成了大部分内容，这里最后补充如何将Tno适配到Autoregressive Language Model(causal)的情形。和Attention类似，只要保证Toeplitz matrix的上三角部分为$0$即可，即：\n",
    "\n",
    "$$\n",
    "\\mathbf T=\\left[\\begin{array}{cccccc}\n",
    "t_0 & 0 & 0 & \\cdots & \\cdots & 0 \\\\\n",
    "t_1 & t_0 & 0 & \\ddots & & \\vdots \\\\\n",
    "\n",
    "t_2 & t_1 & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & 0 & 0 \\\\\n",
    "\\vdots & & \\ddots & t_1 & t_0 &0 \\\\\n",
    "t_{n-1} & \\ldots & \\ldots & t_2 & t_1 & t_0\n",
    "\\end{array}\\right] \\in \\mathbb R^{n\\times n}.\n",
    "$$\n",
    "在实现时，注意到`fft`是zero padding，所以只需要将输入：\n",
    "```python\n",
    "t2 = torch.cat([t_zero, t_pos, t_zero, t_neg], dim=0).cuda()\n",
    "```\n",
    "修改为下式即可：\n",
    "```python\n",
    "t2 = torch.cat([t_zero, t_pos, t_zero], dim=0).cuda()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "在本节中，我们从naive的算法开始，最终得到了一个基于FFT算法的高效实现，并且给出处理单向情形的方案。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rpe的实现\n",
    "注意到Tno的计算涉及到$x,t$，$x$是输入，$t$是相对位置系数，所以下一步就是如何计算$t$。对于序列长度为$n$，特征维度为$d$的模型，我们一共有$(2n-1)\\times d$个系数，所以接下来的问题就是如何得到这些系数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Naive实现\n",
    "最简单的思路就是直接给模型增加$(2n-1)\\times d$个参数，但是这样做有几个问题：\n",
    "1. 当序列长度$n$比较大的时候，模型参数量会非常多；\n",
    "2. 尽管我们有$(2n-1)\\times d$个系数，但是对于每个channel的$2n-1$个系数，不能完全假设他们是独立的，例如$t_1$和$t_{-1}$必然有内在联系；\n",
    "3. 无法处理任意长的序列；\n",
    "   1. 这点可以理解为，当超过最大序列长度时，没有对应的系数，所以模型也没有[外推性](https://arxiv.org/abs/2108.12409)；\n",
    "\n",
    "那么是否有办法解决这些问题呢？回答是肯定的。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Position Encoder\n",
    "\n",
    "对于问题１，２，我们利用某种方式参数化这$(2n-1)\\times d$个参数即可，最简单方式就是使用神经网络，特别的，我们使用的是一个名为Relative Position Encoder(RPE)的网络，网络的输入是1维实数$-(n-1), \\ldots, (n-1)$，输出是$d$维特征。在使用时，我们会输入$[-(n-1),\\ldots, (n-1)]^{\\top} \\in \\mathbb R^{2n-1}$，输出的形状是$(2n-1)\\times d$。\n",
    "\n",
    "对于问题３，我们现在可以一定程度上解决这个问题，现在只要将相对位置（超出训练时的最大训练长度也可）输入到RPE中，即可得到对应系数。但是这样还远远不够，因为这种方式只是让模型“强行”计算了一个值，为了使得性能正常，我们参考了[Alibi](https://arxiv.org/abs/2108.12409)的方案，使用了指数衰减的形式，即：\n",
    "$$\n",
    "    \\bar t_{i-j}=\\lambda^{|i-j|} t_{i-j}, 0< \\lambda < 1.\n",
    "$$\n",
    "其中$\\lambda$是一个超参，我们在$n=512$时选择$\\lambda=0.99$。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Relative Position Encoder\n",
    "\n",
    "有了之前的讨论，我们给出Relative Position Encoder的实现，本质是就是一个全连接网络，加上归一化和激活函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rpe(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        outdim, \n",
    "        residual, \n",
    "        act=\"relu\", \n",
    "        bias=True, \n",
    "        layers=3, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual = residual\n",
    "        self.outdim = outdim\n",
    "        self.pos_dim = dim\n",
    "        self.act = act\n",
    "        self.pos_proj = nn.Linear(1, self.pos_dim, bias=bias)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(layers):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(self.pos_dim),\n",
    "                    self.get_act(),\n",
    "                    nn.Linear(self.pos_dim, self.pos_dim, bias=bias),\n",
    "                )\n",
    "            )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.LayerNorm(self.pos_dim),\n",
    "            self.get_act(),\n",
    "            nn.Linear(self.pos_dim, self.outdim, bias=bias),\n",
    "        )\n",
    "        \n",
    "    def get_act(self):\n",
    "        if self.act == \"silu\":\n",
    "            return nn.SiLU(inplace=True)\n",
    "        else:\n",
    "            return nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, biases):\n",
    "        x = self.pos_proj(biases)\n",
    "        if self.residual:\n",
    "            for m in self.layers:\n",
    "                x = m(x) + x\n",
    "        else:\n",
    "            for m in self.layers:\n",
    "                x = m(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将Tno和Rpe合并\n",
    "\n",
    "在我们的原始实现中，Rpe是和Tno合并在一起的，完整的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tno(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        h, \n",
    "        dim, \n",
    "        rpe_dim, \n",
    "        causal=False, \n",
    "        use_decay=False, \n",
    "        residual=False, \n",
    "        act=\"relu\", \n",
    "        par_type=1, \n",
    "        gamma=0.99,\n",
    "        bias=True,\n",
    "        layers=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h = h\n",
    "        self.dim = dim\n",
    "        self.causal = causal\n",
    "        self.par_type = par_type\n",
    "        self.zero_value = 0\n",
    "        self.use_decay = use_decay\n",
    "        if self.use_decay:\n",
    "            self.gamma = nn.Parameter(torch.ones(h, 1, dim) * gamma, requires_grad=False)\n",
    "\n",
    "        self.rpe = Rpe(\n",
    "            dim=rpe_dim, \n",
    "            outdim=h * dim, \n",
    "            residual=residual,\n",
    "            act=act,\n",
    "            bias=bias, \n",
    "            layers=layers,\n",
    "        )\n",
    "        \n",
    "        if self.causal:\n",
    "            self.forward = self.forward_causal\n",
    "        else:\n",
    "            self.forward = self.forward_non_causal\n",
    "\n",
    "    def get_pos(self, n):\n",
    "        if self.par_type == 1:\n",
    "            index = torch.arange(1, 1 + n).reshape(n, -1) * 1.0\n",
    "        elif self.par_type == 2:\n",
    "            index = torch.arange(1, 1 + n).reshape(n, -1) * 1.0 / n\n",
    "        elif self.par_type == 3:\n",
    "            index = torch.exp(torch.arange(1, 1 + n).reshape(n, -1) * 1.0 / n)\n",
    "        \n",
    "        return index\n",
    "        \n",
    "    def get_zero(self):\n",
    "        index = torch.zeros(1).reshape(1, -1) * 1.0\n",
    "        if self.par_type == 3:\n",
    "            index = torch.exp(index)\n",
    "            \n",
    "        return index\n",
    "\n",
    "    def get_neg(self, n):\n",
    "        if self.causal:\n",
    "            index = torch.ones(self.h * n * self.dim).reshape(self.h, n, self.dim) * self.zero_value\n",
    "        else:\n",
    "            if self.par_type == 1:\n",
    "                index = -torch.arange(1, 1 + n).flip(0).reshape(n, -1) * 1.0\n",
    "            elif self.par_type == 2:\n",
    "                index = -torch.arange(1, 1 + n).flip(0).reshape(n, -1) * 1.0 / n\n",
    "\n",
    "        return index\n",
    "    \n",
    "    def rpe_transform(self, x):\n",
    "        # n, 1 -> n, (d * h)\n",
    "        res = self.rpe(x)\n",
    "        # n, (d * h) -> h, n, d\n",
    "        res = rearrange(res, 'n (h d) -> h n d', h=self.h)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def forward_causal(self, x, dim=-2):\n",
    "        # x: b, h, n, d\n",
    "        n = x.shape[dim]\n",
    "        # a0, a1, ... , a(n-1), a0, a(-(n-1)), ... , a(-1)\n",
    "        ##### coef\n",
    "        # 1, d, 1 -> h, 1, d\n",
    "        zero = self.rpe_transform(self.get_zero().to(x))\n",
    "        pos = self.rpe_transform(self.get_pos(n - 1).to(x))\n",
    "\n",
    "        if self.use_decay:\n",
    "            coef = torch.arange(1, n).reshape(1, -1, 1).to(x)\n",
    "            gamma = self.gamma\n",
    "            gamma = gamma ** coef\n",
    "            pos = gamma * pos\n",
    "        a = torch.cat([zero, pos, zero], dim=1)\n",
    "        a = self.act_fun(a)\n",
    "\n",
    "        # x: b, h, n, d\n",
    "        # a: h, l, d\n",
    "        output = self.compute(x, a, dim, n)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    def forward_non_causal(self, x, dim=-2):\n",
    "        # x: b, h, n, d\n",
    "        n = x.shape[dim]\n",
    "        # a0, a1, ... , a(n-1), a0, a(-(n-1)), ... , a(-1)\n",
    "        ##### coef\n",
    "        # 1, d, 1 -> h, 1, d\n",
    "        zero = self.rpe_transform(self.get_zero().to(x))\n",
    "        pos = self.rpe_transform(self.get_pos(n - 1).to(x))\n",
    "        neg_index = self.get_neg(n - 1).to(x)\n",
    "        if self.causal:\n",
    "            neg = neg_index\n",
    "        else:\n",
    "            neg = self.rpe_transform(neg_index)\n",
    "\n",
    "        if self.use_decay:\n",
    "            coef = torch.arange(1, n).reshape(1, -1, 1).to(x)\n",
    "            gamma = self.gamma\n",
    "            gamma = gamma ** coef\n",
    "            pos = gamma * pos\n",
    "            neg = torch.flip(gamma, dims=[1]) * neg\n",
    "        a = torch.cat([zero, pos, zero, neg], dim=1)\n",
    "        a = self.act_fun(a)\n",
    "        # x: b, h, n, d\n",
    "        # a: h, l, d\n",
    "        output = self.compute(x, a, dim, n)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def compute(self, x, a, dim, n):\n",
    "        # x: b, h, n, d\n",
    "        # a: h, n, d\n",
    "        y = torch.fft.rfft(x, 2 * n, dim=dim)\n",
    "        v = torch.fft.rfft(a, 2 * n, dim=dim).unsqueeze(0)\n",
    "        u = v * y\n",
    "        output = torch.fft.irfft(u, 2 * n, dim=dim)[:, :, :n, :]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tnn layer的实现\n",
    "\n",
    "有了之前的铺垫，我们可以介绍Tnn Layer，该模块包含一个Token mixer(GTU)以及一个Channel mixer(GLU)，由于GLU和GTU非常相似，所以我们从GLU开始介绍。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLU\n",
    "[GLU](https://arxiv.org/abs/2002.05202)是利用Gate的形式达到Channel mixing的作用，写成数学公式为：\n",
    "\n",
    "$$\n",
    "\\mathbf O = [f({\\mathbf X} {\\mathbf W_1}) \\odot ({\\mathbf X} {\\mathbf W_2})] {\\mathbf W_3}.\n",
    "$$\n",
    "\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self, d1, d2, act_fun, fina_act=\"None\", dropout=0.0, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(d1, d2, bias=bias)\n",
    "        self.l2 = nn.Linear(d1, d2, bias=bias)\n",
    "        self.l3 = nn.Linear(d2, d1, bias=bias)\n",
    "        self.act_fun = get_activation_fn(act_fun)\n",
    "        self.p = dropout\n",
    "        if self.p > 0.0:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fina_act = get_activation_fn(fina_act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.l1(x)\n",
    "        weight = self.act_fun(o1)\n",
    "        if self.p > 0.0:\n",
    "            weight = self.dropout(weight)\n",
    "        o2 = self.l2(x)\n",
    "        output = weight * o2\n",
    "        output = self.l3(output)\n",
    "        output = self.fina_act(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GTU\n",
    "\n",
    "GTU参考了GLU的思路，唯一的不同是在其中一个分支上使用了`Tno`，并且增加一个激活函数，写成数学公式即为：\n",
    "$$\n",
    "\\mathbf O = [f({\\mathbf X} {\\mathbf W_1}) \\odot (\\mathbf T f({\\mathbf X} {\\mathbf W_2}))] {\\mathbf W_3}.\n",
    "$$\n",
    "实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gtu(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        bias=True,\n",
    "        act_fun=\"silu\",\n",
    "        causal=False,\n",
    "        expand_ratio=3,\n",
    "        use_norm=False,\n",
    "        norm_type=\"layernorm\",\n",
    "        use_decay=False,\n",
    "        rpe_layers=3,\n",
    "        rpe_embedding=512,\n",
    "        rpe_act=\"relu\",\n",
    "        normalize=False,\n",
    "        par_type=1,\n",
    "        residual=False,\n",
    "        gamma=0.99,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.num_heads = num_heads\n",
    "        self.normalize = normalize\n",
    "\n",
    "        d1 = int(self.expand_ratio * embed_dim)\n",
    "        d1 = (d1 // self.num_heads) * self.num_heads\n",
    "        self.head_dim = d1 // num_heads\n",
    "        # linear projection\n",
    "        self.v_proj = nn.Linear(embed_dim, d1, bias=bias)\n",
    "        self.u_proj = nn.Linear(embed_dim, d1, bias=bias)\n",
    "        self.o = nn.Linear(d1, embed_dim, bias=bias)\n",
    "        self.act = get_activation_fn(act_fun)\n",
    "        # tno\n",
    "        self.toep = Tno(\n",
    "            h=num_heads, \n",
    "            dim=self.head_dim,\n",
    "            rpe_dim=rpe_embedding, \n",
    "            causal=causal, \n",
    "            use_decay=use_decay, \n",
    "            residual=residual,\n",
    "            act=rpe_act,\n",
    "            par_type=par_type,\n",
    "            gamma=gamma,\n",
    "            bias=bias,\n",
    "            layers=rpe_layers,\n",
    "        )\n",
    "        # norm\n",
    "        self.norm_type = norm_type\n",
    "        self.use_norm = use_norm\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: b, n, d\n",
    "        num_heads = self.num_heads\n",
    "\n",
    "        u = self.act(self.u_proj(x))\n",
    "        v = self.act(self.v_proj(x))\n",
    "        # reshape\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=num_heads)\n",
    "        output = self.toep(v, dim=-2, normalize=self.normalize)\n",
    "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
    "        output = u * output\n",
    "        output = self.o(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TnnLayer\n",
    "\n",
    "有了之前的准备工作，我们很容易实现出TnnLayer，因为这只不过是GTU和GLU的堆叠："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TnnLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim, \n",
    "        num_heads,\n",
    "        rpe_embedding,\n",
    "        glu_dim,\n",
    "        # model params\n",
    "        prenorm=True,\n",
    "        norm_type=\"layernorm\",\n",
    "        # gtu params\n",
    "        causal=False,\n",
    "        gtu_act=\"silu\",\n",
    "        expand_ratio=3,\n",
    "        use_decay=False,\n",
    "        gamma=0.999,\n",
    "        # rpe params\n",
    "        rpe_act=\"relu\",\n",
    "        rpe_layers=3,\n",
    "        # glu params\n",
    "        glu_act=\"silu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.token_mixer = Gtu(\n",
    "            # gtu params\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            act_fun=gtu_act,\n",
    "            norm_type=norm_type,\n",
    "            causal=causal,\n",
    "            expand_ratio=expand_ratio,\n",
    "            use_decay=use_decay,\n",
    "            gamma=gamma,\n",
    "            # rpe params\n",
    "            rpe_embedding=rpe_embedding,\n",
    "            rpe_act=rpe_act,\n",
    "            rpe_layers=rpe_layers,\n",
    "        )\n",
    "\n",
    "        self.token_norm = nn.LayerNorm(dim)\n",
    "        self.feature_norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        self.feature_mixer = GLU(\n",
    "            d1=dim, \n",
    "            d2=glu_dim,\n",
    "            act_fun=glu_act,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.token_mixer(self.token_norm(x))\n",
    "        x = x + self.feature_mixer(self.feature_norm(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用时，您只需要将TransformerLayer替换成TnnLayer即可。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "在本节中，我们完成了TnnLayer的实现，有了之前的铺垫工作，这一切并不困难。现在，您已经可以将Tnn应用到您的项目中了。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全文总结\n",
    "\n",
    "通过之前的内容，您应该对TNN有所了解，这里，让我们对全文的核心进行总结：\n",
    "- Transformer可以分为Token mixing和Channel mixing；\n",
    "- Attention的作用是Token mixing，而相对位置信息对Attention很重要，我们提出使用相对位置信息(Toepltiz matrix)来代替Attention Matrix；\n",
    "- 使用Toeplitz matrix进行矩阵乘法可以加速，所以我们的方法理论上速度很快；\n",
    "- Toeplitz matrix的系数可以使用Rpe进行参数化，从而减少参数，结合指数衰减可以得到外推性；\n",
    "\n",
    "当然，TNN还有很多问题存在，例如：\n",
    "- 为什么相对位置信息就足够进行序列建模？\n",
    "- TNN真的只使用了相对位置信息吗？\n",
    "- TNN能达到理论速度上界吗？\n",
    "- TNN不能做哪些任务？\n",
    "- TNN有哪些先验假设？\n",
    "\n",
    "关于这些问题，我们将在后续的博客中回答，期待您的再次阅读。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
