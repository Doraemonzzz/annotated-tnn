
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../en/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.14">
    
    
      
        <title>Zh - The Annotated Tnn</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.85bb2934.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="The Annotated Tnn" class="md-header__button md-logo" aria-label="The Annotated Tnn" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            The Annotated Tnn
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Zh
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="The Annotated Tnn" class="md-nav__button md-logo" aria-label="The Annotated Tnn" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    The Annotated Tnn
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../en/" class="md-nav__link">
        En
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Zh
      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<p><center><h1> The Annotated Tnn </h1></center></p>
<p><center></p>
<p><a href="https://openreview.net/pdf?id=IxmWsm4xrua">Toeplitz Neural Network for Sequence Modeling</a></p>
<p></center>
<img src="../images/network.png" width="100%"/></p>
<p><em>博客由<a href="https://github.com/Doraemonzzz">Doreamonzzz</a>撰写。</em></p>
<p>更新日志：
- 20230313，开始撰写博客；
- 20230320，完成动机以及各个部件的实现部分；
- 20230524，完成校阅以及引用；</p>
<p>Toeplitz Neural Network(TNN)是一种全新的网络结构，以一种完全不同的方式进行序列建模，在单向/双向语言模型，图像分类任务上和Transformer性能相近，并且在长序列建模<a href="https://arxiv.org/abs/2011.04006">LRA</a>任务上取得和<a href="https://arxiv.org/abs/2111.00396">S4</a>相当的性能。这篇博客的主要目的就是以<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>和<a href="https://srush.github.io/annotated-s4/">The Annotated S4</a>风格介绍TNN，在阅读完这篇博客后，您将得到如下收获：
1. 了解TNN的动机和设计理念；
2. 掌握TNN各个部件的实现；</p>
<!-- 3. 学习如何将TNN应用在$n$维序列建模任务；
1. 了解TNN的优缺点；
2. 了解TNN和S4, RWKV等方法的联系； -->

<p>总而言之，在阅读完本博客之后，您将成为TNN的专家，并且可以将TNN应用到您的项目中，让我们开始吧。</p>
<h1 id="_1">目录<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<ul>
<li><a href="#目录">目录</a></li>
<li><a href="#预备知识">预备知识</a></li>
<li><a href="#token-mixing-and-channel-mixing">Token mixing and channel mixing</a></li>
<li><a href="#相对位置编码">相对位置编码</a></li>
<li><a href="#tnn的动机">TNN的动机</a></li>
<li><a href="#tnn的实现">TNN的实现</a></li>
<li><a href="#准备工作">准备工作</a></li>
<li><a href="#tno的实现">Tno的实现</a><ul>
<li><a href="#naive实现">Naive实现</a></li>
<li><a href="#matrix-production实现">Matrix production实现</a></li>
<li><a href="#fft实现">FFT实现</a></li>
<li><a href="#circulant-matrix">Circulant matrix</a></li>
<li><a href="#定义">定义</a></li>
<li><a href="#快速矩阵乘法">快速矩阵乘法</a></li>
<li><a href="#实现">实现</a></li>
<li><a href="#小结">小结</a></li>
<li><a href="#toeplitz-matrix">Toeplitz matrix</a></li>
<li><a href="#定义-1">定义</a></li>
<li><a href="#快速矩阵乘法-1">快速矩阵乘法</a></li>
<li><a href="#实现-1">实现</a></li>
<li><a href="#验证实现">验证实现</a></li>
<li><a href="#补充">补充</a></li>
<li><a href="#小结-1">小结</a></li>
</ul>
</li>
<li><a href="#rpe的实现">Rpe的实现</a><ul>
<li><a href="#naive实现-1">Naive实现</a></li>
<li><a href="#relative-position-encoder">Relative Position Encoder</a></li>
<li><a href="#实现relative-position-encoder">实现Relative Position Encoder</a></li>
<li><a href="#将tno和rpe合并">将Tno和Rpe合并</a></li>
</ul>
</li>
<li><a href="#tnn-layer的实现">Tnn layer的实现</a><ul>
<li><a href="#glu">GLU</a></li>
<li><a href="#gtu">GTU</a></li>
<li><a href="#tnnlayer">TnnLayer</a></li>
<li><a href="#小结-2">小结</a></li>
</ul>
</li>
<li><a href="#全文总结">全文总结</a></li>
</ul>
<h1 id="_2">预备知识<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h1>
<h2 id="token-mixing-and-channel-mixing">Token mixing and channel mixing<a class="headerlink" href="#token-mixing-and-channel-mixing" title="Permanent link">&para;</a></h2>
<p>让我们首先从Transformer开始。Transformer作为一个网络结构已经席卷了各个领域，其核心部分主要可以由如下两个计算公式描述：
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf X_1 &=\mathrm{Norm}(\mathbf X + \mathrm{MHA}(\mathbf X)),\\
\mathbf O &= \mathrm{Norm}(\mathbf X_1 + \mathrm{FFN}(\mathbf X_1)).
\end{aligned}
</script>
其中$\mathbf X \in \mathbb R^{n\times d}$是输入（也可以称为token matrix，其中矩阵的每一行为一个token的向量表示），$n$是序列长度，$d$是特征维度。</p>
<p>既然现在有两个主要模块——$\mathrm {MHA}$和$\mathrm {FFN}$，那么他们的作用是否有所不同呢？在<a href="https://arxiv.org/abs/2111.11418">Metaformer</a>一文中，研究者指出，$\mathrm {MHA}$的主要作用是Token mixing，而$\mathrm {FFN}$的主要作用是Channel mixing。</p>
<p>这是什么意思呢？我们可以从矩阵乘法的角度清晰的理解这点：给定输入（token matrix）$\mathbf X \in \mathbb R^{n\times d}$，考虑矩阵乘法$\mathbf A \mathbf X$和$\mathbf X \mathbf B$，那么：
- $\mathbf A \mathbf X$表示矩阵$\mathbf X$行的线性组合，而每一行表示一个token，即token的线性组合，所以称为token mixing；
- $\mathbf X  \mathbf B$表示矩阵$\mathbf X$列的线性组合，而每一列表示一个channel，即channel的线性组合，所以称为channel mixing；</p>
<p>在Transformer中，矩阵$\mathbf A$即为$\mathrm{Softmax}(\mathbf Q \mathbf K^{\top} /\sqrt{d})$，矩阵$\mathbf B$即为$\mathrm {FFN}$中的全连接层。</p>
<p>大多数对Transformer的改进都是集中在token mixing:$\mathbf A \mathbf X$的计算上，以各种各样的方式降低其运算复杂度，TNN也是使用了类似的思路，最核心的一点就是利用了相对位置编码，或者说，Toeplitz矩阵。</p>
<h2 id="_3">相对位置编码<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>位置编码是Transformer中的重要组成部分，一开始广为使用的是<a href="https://arxiv.org/abs/1706.03762">绝对位置编码(APE)</a>，这种编码的方式可以用如下计算方式概括：
<script type="math/tex; mode=display">
\mathbf x_i =\mathbf w_i + \mathbf p_i.
</script>
其中$\mathbf w_i$表示第$i$个词的word embedding，$\mathbf p_i$表示第$i$个位置的position embedding。</p>
<p>后来，有研究人员发现，在序列建模中，词的相对位置信息，可能比词的绝位置信息更加重要。</p>
<blockquote>
<p>例如"我年纪比你大"的语意和"我年纪比你大"完全不同，但是这两句话只是交换了"你"和"我"的位置。
</p>
</blockquote>
<p>于是研究人员开始将相对位置编码引入，相对位置编码的使用和绝对位置编码有所不同，其作用在Attention计算的位置：
<script type="math/tex; mode=display">
\mathbf s_{ij} = \mathbf q_i^{\top} \mathbf k_j/\sqrt{d} + t_{i-j}.
</script>
如果写成矩阵的形式则更加直观：
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf S & = \mathbf Q \mathbf K^{\top} / \sqrt {d} + \mathbf T,\\
\mathbf T & =\left[\begin{matrix}
t_0 & t_{-1} & \cdots  & t_{-n+1} \\
t_1 & t_0  &  &  \vdots \\
\vdots &   &   t_0 & t_{-1} \\
t_{n-1} & \ldots  & t_1 & t_0
\end{matrix}\right] \in \mathbb R^{n\times n}.
\end{aligned}
</script>
</p>
<p>这里，矩阵$\mathbf T$有一个数学名称——<a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz矩阵</a>，不难看出该矩阵有$2n-1$个独立元素。</p>
<h1 id="tnn">TNN的动机<a class="headerlink" href="#tnn" title="Permanent link">&para;</a></h1>
<p>有了之前的准备工作，可以引入我们工作的两个动机：
1. 既然相对位置信息如此重要，那么有没有可能只依赖于相对位置信息（Toeplitz matrix）进行token mixing呢？
   1. 直观上来说，就是将Attention Matrix替换为Toeplitz matrix。
2. 假设(1)成立，那么我们需要进行的主要操作是$\mathbf T \mathbf X$，既然矩阵$\mathbf T$是一个特殊结构的矩阵，那么有没有可能加速运算呢？</p>
<p>我们对两个问题都进行了肯定的答复：
1. 完全可以只依赖于相对位置信息进行token mixing；
2. 由于矩阵的特殊性，可以将运算复杂度由$O(n^2 d)$降低为$O(nd\log n)$；</p>
<p>可以看到，我们的动机极其简单和优雅，最核心的思路就是将$\mathrm{Softmax}(\mathbf Q \mathbf K^{\top} / \sqrt {d})$替换为$\mathbf T$，但是，这种简单的替换就可以拥有比各种花哨更改更好的性能，这就更加验证了相对位置信息在序列建模中的重要性。</p>
<!-- 因此，在后续的讨论快速矩阵乘法时，我们指的是及$\mathbf T\mathbf x$，其中$\mathbf T\in \mathbb R^{n\times n}, \mathbf x \in \mathbb R^{n\times 1}$。 -->

<h1 id="tnn_1">TNN的实现<a class="headerlink" href="#tnn_1" title="Permanent link">&para;</a></h1>
<h2 id="_4">准备工作<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>接下来的问题就是如何实现TNN，在此之前，我们对之前的公式做一定的调整。</p>
<p>在之前的讨论中，我们提到了$\mathbf T \mathbf X$可以高效实现，其中$\mathbf T\in \mathbb R^{n\times n}, \mathbf X \in \mathbb R^{n\times d}$，这种情况相当于每个channel共享同一个Toeplitz matrix，但是注意到我们可以让不同的channel使用不同的Toeplitz matrix，我们经验上发现，这样一定程度上可以增大模型的表达性，所以在TNN中，<strong>每个channel</strong>使用了不同的Toeplitz matrix。注意到形状为$n\times n$的Toeplitz matrix实际上只有$2n-1$个独立元素，为了方便后续讨论，我们定义如下映射：$f: \mathbb R^{(2n-1)\times 1} \to \mathbb R^{n\times n}$：
<script type="math/tex; mode=display">
f(\mathbf t)=f(t_{-n+1},\ldots, t_{n-1}) =\left[\begin{matrix}
t_0 & t_{-1} & \cdots  & t_{-n+1} \\
t_1 & t_0  &  &  \vdots \\
\vdots &   &   t_0 & t_{-1} \\
t_{n-1} & \ldots  & t_1 & t_0
\end{matrix}\right] \in \mathbb R^{n\times n}.
</script>
该映射的作用是将维度为$(2n-1)\times 1$的向量填充为$n\times n$的Toeplitz matrix。</p>
<p>结合之前的记号，我们定义为Tno算子(Toeplitz neural operator)为：
<script type="math/tex; mode=display">
\mathrm{Tno}: \mathbb R^{(2n-1)\times d}\times \mathbb R^{n\times d} \to \mathbb R^{n\times d},\\
\mathbf O= \mathrm{Tno}(\mathbf T, \mathbf X), \\
\mathbf O[:, i]= f(\mathbf T[:, i]) \mathbf X[:, i].
</script>
</p>
<p>备注：这里的记号$\mathbf T\in \mathbb R^{(2n-1)\times d}$和一开始含义有所不同，注意不要搞混。</p>
<p>在开始正式的实现之前，我们先引入一些必要的依赖库以及一些辅助函数：</p>
<!-- ，写成计算公式即为：
$$
\mathbf O[:, i]= \mathbf T[:, i] \mathbf X[:, i], \\
\mathbf O[:, i]\in \mathbb R^{n\times 1}, \mathbf T[:, i]\in \mathbb R^{n\times n},  \mathbf X[:, i]\in \mathbb R^{n\times 1}.
$$ -->

<!-- 备注：这里记号有点太严谨，$\mathbf T[:, i]$的形状应该是$(2n-1) \times 1$，上述符号是指 -->

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

def get_activation_fn(activation):
    if activation == &quot;gelu&quot;:
        return F.gelu
    elif activation == &quot;relu&quot;:
        return F.relu
    elif activation == &quot;elu&quot;:
        return F.elu
    elif activation == &quot;sigmoid&quot;:
        return F.sigmoid
    elif activation == &quot;exp&quot;:
        return torch.exp
    elif activation == &quot;leak&quot;:
        return F.leaky_relu
    elif activation == &quot;1+elu&quot;:
        def f(x):
            return 1 + F.elu(x)
        return f
    elif activation == &quot;2+elu&quot;:
            def f(x):
                return 2 + F.elu(x)
            return f
    elif activation == &quot;silu&quot;:
        return F.silu
    else:
        return lambda x: x
</code></pre>
<h2 id="tno">Tno的实现<a class="headerlink" href="#tno" title="Permanent link">&para;</a></h2>
<h3 id="naive">Naive实现<a class="headerlink" href="#naive" title="Permanent link">&para;</a></h3>
<p>最朴素的实现自然是利用定义进行实现，例如如下代码中，我们使用4重循环，外面两重循环遍历batch, channel维度，第三重循环遍历输出位置，最后一重循环遍历求和项，注意到我们的$\mathbf T[:, i]$输入形式为$t_{-n+1}, ... , t_{-1}, t_0, t_1, ... , t_{n - 1}$，第三重循环遍历到$i$时，涉及的$t$为$t_{i}, t_{i-1},\ldots, t_{i-n+1}$，而$n - 1 + i$是$t_{i}$在$\mathbf T[:, i]$的实际索引：</p>
<pre><code class="language-python">def tno_naive(x, t):
    # x: (b, n, d)
    # t: (2n - 1, d), t_(-(n - 1)), ... , t_(-1), t_0, t_1, ... , t_(n - 1) 
    b, n, d = x.shape
    o = torch.zeros_like(x).to(x)
    for b_ in range(b):
        for d_ in range(d):
            for i in range(n):
                for j in range(n):
                    o[b_][i][d_] += t[n - 1 + i - j][d_] * x[b_][j][d_]

    return o
</code></pre>
<p>这种实现显然太低效，但是至少我们有了一个正确的版本，这对我们后续改进算法也是有帮助的，不难看出这样计算的时间复杂度为$O(n^2d)$，空间复杂度为$O(nd)$（忽略batch维度）。                             </p>
<h3 id="matrix-production">Matrix production实现<a class="headerlink" href="#matrix-production" title="Permanent link">&para;</a></h3>
<p>第二种实现是并行版本，其思路就是先构造Toeplitz matrix，然后利用矩阵乘法进行计算。最主要的部分是将映射$f$实现出来，代码基于<a href="https://stackoverflow.com/questions/69809789/is-there-any-way-to-create-a-tensor-with-a-specific-pattern-in-pytorch">此处</a>，主要思路是先将输入改写为$t_0, t_{-1}, ... , t_{1-n}, t_{n - 1}, ... , t_1$，然后构造index $0, 1, \ldots,n -1, -(n - 1), ..., -1$，将输入映射到Toeplitz matrix，最后得到Toeplitz matrix进行矩阵乘法：</p>
<pre><code class="language-python">def tno_matrix(x, t):
    # x: (b, n, d)
    # t: (2n - 1, d), t_(-(n - 1)), ... , t_(-1), t_0, t_1, ... , t_(n - 1) 
    n = x.shape[1]
    t = t.unsqueeze(0)
    # c: t_0, t_1, ... , t_(n - 1)
    c = t[:, n - 1:]
    # r: t_0, t_(-1), ... , t_(-(n - 1))
    r = t[:, :n].flip(1)
    # vals: [t_0, t_(-1), ... , t_(-(n - 1)), t_(n - 1), ... , t_1]
    vals = torch.cat([r, c[:, 1:].flip(1)], dim=-2)
    i, j = torch.ones(n, n).nonzero().T
    t_matrix = vals[:, j - i].reshape(n, n, -1)
    o = torch.einsum(&quot;n m d, b m d -&gt; b n d&quot;, t_matrix, x)

    return o
</code></pre>
<p>这种实现的好处是可以利用矩阵乘法，尽管复杂度依然为$O(n^2d)$，但实际效率会快很多；但是由于要构造Toeplitz matrix，所以空间复杂度为$O(n^2d)$，并且这部分还是一个很大的IO开销，所以实际中的速度并不会很快。</p>
<h3 id="fft">FFT实现<a class="headerlink" href="#fft" title="Permanent link">&para;</a></h3>
<p>有了之前的铺垫，可以看出前两种方法无论是时间复杂度和空间复杂度相比Attention并没有什么优势，那么有没有办法解决这点呢？回答是肯定的，这就需要<a href="">FFT</a>这把利刃。后续的讨论涉及到一些数学知识，这里先高度概括一下思路：
1. 给出Circulant matrix的快速矩阵乘法算法；
2. 建立Toeplitz marix和Circulant matrix的关系；</p>
<h3 id="circulant-matrix">Circulant matrix<a class="headerlink" href="#circulant-matrix" title="Permanent link">&para;</a></h3>
<h4 id="_5">定义<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<p>矩阵$\mathbf C\in \mathbb R^{n\times n}$是一个<a href="https://en.wikipedia.org/wiki/Circulant_matrix">Circulant matrix</a>当且仅当$\mathbf C_{ij}= c_{(i-j + n )\bmod n}$ ,即：
<script type="math/tex; mode=display">
\mathbf C=\left[\begin{matrix}
c_0 & c_{n-1} &c_{n-2} & \cdots & \cdots & c_{1} \\
c_1 & c_0 & c_{n-1} & \ddots & & \vdots \\
c_2 & c_1 & \ddots & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & c_{n-1} & c_{n-2} \\
\vdots & & \ddots & c_1 & c_0 & c_{n-1} \\
c_{n-1} & \ldots & \ldots & c_2 & c_1 & c_0
\end{matrix}\right] \in \mathbb R^{n\times n}.
</script>
关于Circulant matrix，有如下重要性质：</p>
<p>Circulant matrix $\mathbf C\in \mathbb R^{n\times n}$正交相似于对角阵$\mathbf \Lambda$，特别地，相似矩阵$\mathbf F$是$n\times n$ DFT矩阵:
<script type="math/tex; mode=display">
\mathbf C = \mathbf F^{\top} \Lambda \mathbf F, \\
\Lambda = \mathrm{diag}\{\mathbf F[c_0,c_1,\ldots, c_{n-1}]^\top\} \in \mathbb R^{n\times n}, 
{\mathbf F}_{st}= \exp\left(\frac{2\pi st i}{n}\right),i^2=-1.
</script>
证明可以参考<a href="https://ee.stanford.edu/~gray/toeplitz.pdf">这里</a>。</p>
<h4 id="_6">快速矩阵乘法<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<p>现在考虑matrix-vector production操作$\mathbf M \mathbf x, \mathbf M\in \mathbb R^{n\times n}, \mathbf x\in \mathbb R^{n\times 1}$，那么：</p>
<ul>
<li>如果$\mathbf M$为一般的矩阵，那么该计算的时间复杂度为$O(n^2)$;</li>
<li>如果$\mathbf M$为DFT矩阵，那么该计算的时间复杂度为$O(n \log n)$;</li>
</ul>
<p>基于上述事实，考虑$\mathbf M=\mathbf C$为Circulant matrix的情形，那么：
<script type="math/tex; mode=display">
\mathbf C \mathbf x = \mathbf F^{\top} \Lambda \mathbf F \mathbf x.
</script>
该计算可以分解为几个步骤：</p>
<ul>
<li>$\mathbf x_{\mathrm{fft}}=\mathbf{Fx}$；</li>
<li>$\mathbf c_{\mathrm{fft}}=\mathbf F[c_0,c_1,\ldots, c_{n-1}]^\top$；</li>
<li>$\mathbf o_{\mathrm{fft}}=\mathbf x_{\mathrm{fft}}\odot \mathbf c_{\mathrm{fft}}$；</li>
<li>$\mathbf o= \mathbf F^{\top} \mathbf o_{\mathrm{fft}}$；</li>
</ul>
<p>其中$\odot$表示element-wise production，可以看出，算法的总时间复杂度为$O(n\log n)$，空间复杂度为$O(n)$，所以Circulant matrix对应的矩阵乘法是高效的。</p>
<h4 id="_7">实现<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<p>有了之前的说明，不难利用<code>fft</code>实现上述计算：</p>
<pre><code class="language-python">def circulant_fft(x, c):
    # x: (b, n, d)
    # c: (n, d), c_0, c_1, ... , c_(n - 1) 
    n = x.shape[1]
    c = c.unsqueeze(0)
    x_fft = torch.fft.rfft(x, n, dim=-2)
    c_fft = torch.fft.rfft(c, n, dim=-2)
    o_fft = x_fft * c_fft
    o = torch.fft.irfft(o_fft, n, dim=-2)

    return o
</code></pre>
<h4 id="_8">小结<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<p>现在我们已经有了一个关于Circulant matrix的高效矩阵乘法，那么下一个问题就是建立Toeplitz matrix和Circulant matrix的关系。</p>
<h3 id="toeplitz-matrix">Toeplitz matrix<a class="headerlink" href="#toeplitz-matrix" title="Permanent link">&para;</a></h3>
<h4 id="_9">定义<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<p>矩阵$\mathbf T\in \mathbb R^{n\times n}$是一个Toeplitz matrix当且仅当$\mathbf T_{ij}= t_{i-j}$，即
<script type="math/tex; mode=display">
\mathbf T=\left[\begin{matrix}
t_0 & t_{-1} &t_{-2} & \cdots & \cdots & t_{-n+1} \\
t_1 & t_0 & t_{-1} & \ddots & & \vdots \\
t_2 & t_1 & \ddots & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & t_{-1} & t_{n-2} \\
\vdots & & \ddots & t_1 & t_0 & t_{-1} \\
t_{n-1} & \ldots & \ldots & t_2 & t_1 & t_0
\end{matrix}\right] \in \mathbb R^{n\times n}.
</script>
从形式上来看，Toeplitz matrix和Circulant matrix非常像，唯一的区别在于前者的独立元素数量为$2n-1$，后者的独立元素数量为$n$，那么一个简单的思路就是将Toeplitz matrix嵌入到一个阶数大于等于$2n-1$矩阵中，而这个矩阵本生是一个Circulant matrix，下面来看下这是如何具体操作的。</p>
<p>可以将Toeplitz matrix $\mathbf T\in \mathbb R^{n\times }$嵌入到Circulant matrix $\mathbf C \in \mathbb R^{2n\times 2n}$中:
<script type="math/tex; mode=display">
c_{k} =\begin{cases}
t_k , 0 \le k \le n - 1\\
t_0 , k=n\\
t_{k -2n},   n+1\le k \le 2n-1
\end{cases} ,
</script>
即，
<script type="math/tex; mode=display">
\mathbf C=\left[\begin{array}{ccccc|ccccc}
t_0 & t_{-1} & \ldots & \ldots & t_{-n+1} & t_0 & t_{n-1} & \ldots & t_2 & t_1 \\
t_1 & t_0 & \ddots & & \vdots & t_{-n+1} & \ddots & \ddots & & t_2 \\
t_2 & \ddots & \ddots & \ddots & \vdots & \vdots & \ddots & & \ddots & \vdots \\
\vdots & & \ddots & t_0 & t_{-1} & t_{-2} & & \ddots & \ddots & t_{n-1} \\
t_{n-1} & \ldots & \ldots & t_1 & t_0 & t_{-1} & t_{-2} & \ldots & t_{-n+1} & t_0 \\
\hline t_0 & t_{n-1} & \ldots & \ldots & t_1 & t_0 & t_{-1} & \ldots & \ldots & t_{-n+1} \\
t_{-n+1} & \ddots & \ddots & & t_2 & t_1 & t_0 & \ddots & & \vdots \\
\vdots & \ddots & & \ddots & \vdots & t_2 & \ddots & \ddots & \ddots & \vdots \\
t_{-2} & & \ddots & \ddots & t_{n-1} & \vdots & & \ddots & t_0 & t_{-1} \\
t_{-1} & t_{-2} & \ldots & \ldots & t_0 & t_{n-1} & \ldots & \ldots & t_1 & t_0
\end{array}\right] \in \mathbb R^{2n\times 2n}.
</script>
使用分块矩阵的符号，我们可以定义：
<script type="math/tex; mode=display">
\begin{gathered}
 \mathbf C = \left[\begin{matrix}
\mathbf C_1 & \mathbf C_2\\
\mathbf C_3 & \mathbf C_4\\
\end{matrix}\right] \in \mathbb R^{2n\times 2n},\mathbf C_s \in \mathbb R^{n \times n}, s=1,2,3,4,
\mathbf C_1 = \mathbf T 
\end{gathered}.
</script>
有了上述准备工作，可以得到Toeplitz matrix-vector production的快速算法。</p>
<h4 id="_10">快速矩阵乘法<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<p>对于向量$\mathbf x\in \mathbb R^{n}$, 定义:
<script type="math/tex; mode=display">
\mathbf x_1 = \left[\begin{matrix}
\mathbf x\\
\mathbf 0_n
\end{matrix}\right] \in \mathbb R^{2n},
</script>
所以，
<script type="math/tex; mode=display">
\mathbf C \mathbf x_1 =\left[\begin{matrix}
\mathbf C_1 & \mathbf C_2\\
\mathbf C_3 & \mathbf C_4\\
\end{matrix}\right]\left[\begin{matrix}
\mathbf x\\
\mathbf 0_n
\end{matrix}\right]=\left[\begin{matrix}
\mathbf C_1 \mathbf x\\
\mathbf C_3 \mathbf x
\end{matrix}\right]=\left[\begin{matrix}
\mathbf T \mathbf x\\
\mathbf C_3 \mathbf x
\end{matrix}\right] \in \mathbb R^{2n},
</script>
因此:
<script type="math/tex; mode=display">
\left[\begin{matrix}
{\mathbf I}_n &
{\mathbf 0}_{n\times n}
\end{matrix}\right]\mathbf C \mathbf x_1  =
\left[\begin{matrix}
\mathbf I_n &
\mathbf 0_{n\times n}
\end{matrix}\right]\left[\begin{array}{c}
\mathbf T \mathbf x\\
\mathbf C_3 \mathbf x
\end{array}\right]=\mathbf T \mathbf x.
</script>
关于时间复杂度，注意到我们是将$n\times n$的Toeplitz matrix嵌入到一个$2n\times 2n$的Circulant matrix中，所以时间复杂度仍然为$O(n\log n)$。</p>
<h4 id="_11">实现<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h4>
<p>和Circulant matrix的情形类似，可以利用<code>fft</code>实现上述计算：</p>
<pre><code class="language-python">def tno_fft(x, t):
    # x: (b, n, d)
    # t: (2 * n, d), t0, t1, ..., t(n-1), t0, t_(-(n-1)), ... , t_(-1)
    n = x.shape[1]
    t = t.unsqueeze(0)
    x_fft = torch.fft.rfft(x, 2 * n, dim=-2)
    t_fft = torch.fft.rfft(t, 2 * n, dim=-2)
    o_fft = x_fft * t_fft
    o = torch.fft.irfft(o_fft, 2 * n, dim=-2)[:, :n]

    return o
</code></pre>
<h3 id="_12">验证实现<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<p>在之前的讨论中，我们给出了Tno的三种实现方式，在本节中，我们将验证这些实现的正确性。</p>
<pre><code class="language-python">b = 2
n = 16
d = 128

t_zero = torch.randn(1, d)
# t1, ..., t(n-1)
t_pos = torch.randn(n - 1, d)
# t-(n-1), ... , t-1
t_neg = torch.randn(n - 1, d)
t1 = torch.cat([t_neg, t_zero, t_pos], dim=0).cuda()
t2 = torch.cat([t_zero, t_pos, t_zero, t_neg], dim=0).cuda()
x = torch.randn(b, n, d).cuda()

o1 = tno_naive(x, t1)
o2 = tno_matrix(x, t1)
o3 = tno_fft(x, t2)

print(f&quot;The output error between tno_naive and tno_matrix is {torch.norm(o1 - o2)}&quot;)
print(f&quot;The output error between tno_naive and tno_matrix is {torch.norm(o1 - o3)}&quot;)
</code></pre>
<pre><code>The output error between tno_naive and tno_matrix is 2.414959999441635e-05
The output error between tno_naive and tno_matrix is 5.38119456905406e-05
</code></pre>
<h3 id="_13">补充<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h3>
<p>现在我们已经完成了大部分内容，这里最后补充如何将Tno适配到Autoregressive Language Model(causal)的情形。和Attention类似，只要保证Toeplitz matrix的上三角部分为$0$即可，即：
$$
\mathbf T=\left[\begin{matrix}
t_0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cdots &amp; 0 \
t_1 &amp; t_0 &amp; 0 &amp; \ddots &amp; &amp; \vdots \</p>
<p>t_2 &amp; t_1 &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \
\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; 0 &amp; 0 \
\vdots &amp; &amp; \ddots &amp; t_1 &amp; t_0 &amp;0 \
t_{n-1} &amp; \ldots &amp; \ldots &amp; t_2 &amp; t_1 &amp; t_0
\end{matrix}\right] \in \mathbb R^{n\times n}.
$$
在实现时，注意到<code>fft</code>是zero padding，所以只需要将输入：</p>
<pre><code class="language-python">t2 = torch.cat([t_zero, t_pos, t_zero, t_neg], dim=0).cuda()
</code></pre>
<p>修改为下式即可：</p>
<pre><code class="language-python">t2 = torch.cat([t_zero, t_pos, t_zero], dim=0).cuda()
</code></pre>
<h3 id="_14">小结<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h3>
<p>在本节中，我们从naive的算法开始，最终得到了一个基于FFT算法的高效实现，并且给出处理单向情形的方案。</p>
<h2 id="rpe">Rpe的实现<a class="headerlink" href="#rpe" title="Permanent link">&para;</a></h2>
<p>注意到Tno的计算涉及到$x,t$，$x$是输入，$t$是相对位置系数，所以下一步就是如何计算$t$。对于序列长度为$n$，特征维度为$d$的模型，我们一共有$(2n-1)\times d$个系数，所以接下来的问题就是如何得到这些系数。</p>
<h3 id="naive_1">Naive实现<a class="headerlink" href="#naive_1" title="Permanent link">&para;</a></h3>
<p>最简单的思路就是直接给模型增加$(2n-1)\times d$个参数，但是这样做有几个问题：
1. 当序列长度$n$比较大的时候，模型参数量会非常多；
2. 尽管我们有$(2n-1)\times d$个系数，但是对于每个channel的$2n-1$个系数，不能完全假设他们是独立的，例如$t_1$和$t_{-1}$必然有内在联系；
3. 无法处理任意长的序列；
   1. 这点可以理解为，当超过最大序列长度时，没有对应的系数，所以模型也没有<a href="https://arxiv.org/abs/2108.12409">外推性</a>；</p>
<p>那么是否有办法解决这些问题呢？回答是肯定的。</p>
<h3 id="relative-position-encoder">Relative Position Encoder<a class="headerlink" href="#relative-position-encoder" title="Permanent link">&para;</a></h3>
<p>对于问题１，２，我们利用某种方式参数化这$(2n-1)\times d$个参数即可，最简单方式就是使用神经网络，特别的，我们使用的是一个名为Relative Position Encoder(RPE)的网络，网络的输入是1维实数$-(n-1), \ldots, (n-1)$，输出是$d$维特征。在使用时，我们会输入$[-(n-1),\ldots, (n-1)]^{\top} \in \mathbb R^{2n-1}$，输出的形状是$(2n-1)\times d$。</p>
<p>对于问题３，我们现在可以一定程度上解决这个问题，现在只要将相对位置（超出训练时的最大训练长度也可）输入到RPE中，即可得到对应系数。但是这样还远远不够，因为这种方式只是让模型“强行”计算了一个值，为了使得性能正常，我们参考了<a href="https://arxiv.org/abs/2108.12409">Alibi</a>的方案，使用了指数衰减的形式，即：
<script type="math/tex; mode=display">
    \bar t_{i-j}=\lambda^{|i-j|} t_{i-j}, 0< \lambda < 1.
</script>
其中$\lambda$是一个超参，我们在$n=512$时选择$\lambda=0.99$。</p>
<h3 id="relative-position-encoder_1">实现Relative Position Encoder<a class="headerlink" href="#relative-position-encoder_1" title="Permanent link">&para;</a></h3>
<p>有了之前的讨论，我们给出Relative Position Encoder的实现，本质是就是一个全连接网络，加上归一化和激活函数：</p>
<pre><code class="language-python">class Rpe(nn.Module):
    def __init__(
        self, 
        dim, 
        outdim, 
        residual, 
        act=&quot;relu&quot;, 
        bias=True, 
        layers=3, 
    ):
        super().__init__()

        self.residual = residual
        self.outdim = outdim
        self.pos_dim = dim
        self.act = act
        self.pos_proj = nn.Linear(1, self.pos_dim, bias=bias)
        self.layers = nn.ModuleList([])
        for i in range(layers):
            self.layers.append(
                nn.Sequential(
                    nn.LayerNorm(self.pos_dim),
                    self.get_act(),
                    nn.Linear(self.pos_dim, self.pos_dim, bias=bias),
                )
            )
        self.out = nn.Sequential(
            nn.LayerNorm(self.pos_dim),
            self.get_act(),
            nn.Linear(self.pos_dim, self.outdim, bias=bias),
        )

    def get_act(self):
        if self.act == &quot;silu&quot;:
            return nn.SiLU(inplace=True)
        else:
            return nn.ReLU(inplace=True)

    def forward(self, biases):
        x = self.pos_proj(biases)
        if self.residual:
            for m in self.layers:
                x = m(x) + x
        else:
            for m in self.layers:
                x = m(x)
        x = self.out(x)

        return x
</code></pre>
<h3 id="tnorpe">将Tno和Rpe合并<a class="headerlink" href="#tnorpe" title="Permanent link">&para;</a></h3>
<p>在我们的原始实现中，Rpe是和Tno合并在一起的，完整的实现如下：</p>
<pre><code class="language-python">class Tno(nn.Module):
    def __init__(
        self, 
        h, 
        dim, 
        rpe_dim, 
        causal=False, 
        use_decay=False, 
        residual=False, 
        act=&quot;relu&quot;, 
        par_type=1, 
        gamma=0.99,
        bias=True,
        layers=3,
    ):
        super().__init__()

        self.h = h
        self.dim = dim
        self.causal = causal
        self.par_type = par_type
        self.zero_value = 0
        self.use_decay = use_decay
        if self.use_decay:
            self.gamma = nn.Parameter(torch.ones(h, 1, dim) * gamma, requires_grad=False)

        self.rpe = Rpe(
            dim=rpe_dim, 
            outdim=h * dim, 
            residual=residual,
            act=act,
            bias=bias, 
            layers=layers,
        )

        if self.causal:
            self.forward = self.forward_causal
        else:
            self.forward = self.forward_non_causal

    def get_pos(self, n):
        if self.par_type == 1:
            index = torch.arange(1, 1 + n).reshape(n, -1) * 1.0
        elif self.par_type == 2:
            index = torch.arange(1, 1 + n).reshape(n, -1) * 1.0 / n
        elif self.par_type == 3:
            index = torch.exp(torch.arange(1, 1 + n).reshape(n, -1) * 1.0 / n)

        return index

    def get_zero(self):
        index = torch.zeros(1).reshape(1, -1) * 1.0
        if self.par_type == 3:
            index = torch.exp(index)

        return index

    def get_neg(self, n):
        if self.causal:
            index = torch.ones(self.h * n * self.dim).reshape(self.h, n, self.dim) * self.zero_value
        else:
            if self.par_type == 1:
                index = -torch.arange(1, 1 + n).flip(0).reshape(n, -1) * 1.0
            elif self.par_type == 2:
                index = -torch.arange(1, 1 + n).flip(0).reshape(n, -1) * 1.0 / n

        return index

    def rpe_transform(self, x):
        # n, 1 -&gt; n, (d * h)
        res = self.rpe(x)
        # n, (d * h) -&gt; h, n, d
        res = rearrange(res, 'n (h d) -&gt; h n d', h=self.h)

        return res

    def forward_causal(self, x, dim=-2):
        # x: b, h, n, d
        n = x.shape[dim]
        # a0, a1, ... , a(n-1), a0, a(-(n-1)), ... , a(-1)
        ##### coef
        # 1, d, 1 -&gt; h, 1, d
        zero = self.rpe_transform(self.get_zero().to(x))
        pos = self.rpe_transform(self.get_pos(n - 1).to(x))

        if self.use_decay:
            coef = torch.arange(1, n).reshape(1, -1, 1).to(x)
            gamma = self.gamma
            gamma = gamma ** coef
            pos = gamma * pos
        a = torch.cat([zero, pos, zero], dim=1)
        a = self.act_fun(a)

        # x: b, h, n, d
        # a: h, l, d
        output = self.compute(x, a, dim, n)

        return output

    def forward_non_causal(self, x, dim=-2):
        # x: b, h, n, d
        n = x.shape[dim]
        # a0, a1, ... , a(n-1), a0, a(-(n-1)), ... , a(-1)
        ##### coef
        # 1, d, 1 -&gt; h, 1, d
        zero = self.rpe_transform(self.get_zero().to(x))
        pos = self.rpe_transform(self.get_pos(n - 1).to(x))
        neg_index = self.get_neg(n - 1).to(x)
        if self.causal:
            neg = neg_index
        else:
            neg = self.rpe_transform(neg_index)

        if self.use_decay:
            coef = torch.arange(1, n).reshape(1, -1, 1).to(x)
            gamma = self.gamma
            gamma = gamma ** coef
            pos = gamma * pos
            neg = torch.flip(gamma, dims=[1]) * neg
        a = torch.cat([zero, pos, zero, neg], dim=1)
        a = self.act_fun(a)
        # x: b, h, n, d
        # a: h, l, d
        output = self.compute(x, a, dim, n)

        return output

    def compute(self, x, a, dim, n):
        # x: b, h, n, d
        # a: h, n, d
        y = torch.fft.rfft(x, 2 * n, dim=dim)
        v = torch.fft.rfft(a, 2 * n, dim=dim).unsqueeze(0)
        u = v * y
        output = torch.fft.irfft(u, 2 * n, dim=dim)[:, :, :n, :]

        return output

</code></pre>
<h2 id="tnn-layer">Tnn layer的实现<a class="headerlink" href="#tnn-layer" title="Permanent link">&para;</a></h2>
<p>有了之前的铺垫，我们可以介绍Tnn Layer，该模块包含一个Token mixer(GTU)以及一个Channel mixer(GLU)，由于GLU和GTU非常相似，所以我们从GLU开始介绍。</p>
<h3 id="glu">GLU<a class="headerlink" href="#glu" title="Permanent link">&para;</a></h3>
<p><a href="https://arxiv.org/abs/2002.05202">GLU</a>是利用Gate的形式达到Channel mixing的作用，写成数学公式为：
<script type="math/tex; mode=display">
\mathbf O = [f({\mathbf X} {\mathbf W_1}) \odot ({\mathbf X} {\mathbf W_2})] {\mathbf W_3}.
</script>
实现如下：</p>
<pre><code class="language-python">class GLU(nn.Module):
    def __init__(self, d1, d2, act_fun, fina_act=&quot;None&quot;, dropout=0.0, bias=True):
        super().__init__()

        self.l1 = nn.Linear(d1, d2, bias=bias)
        self.l2 = nn.Linear(d1, d2, bias=bias)
        self.l3 = nn.Linear(d2, d1, bias=bias)
        self.act_fun = get_activation_fn(act_fun)
        self.p = dropout
        if self.p &gt; 0.0:
            self.dropout = nn.Dropout(p=dropout)
        self.fina_act = get_activation_fn(fina_act)

    def forward(self, x):
        o1 = self.l1(x)
        weight = self.act_fun(o1)
        if self.p &gt; 0.0:
            weight = self.dropout(weight)
        o2 = self.l2(x)
        output = weight * o2
        output = self.l3(output)
        output = self.fina_act(output)

        return output
</code></pre>
<h3 id="gtu">GTU<a class="headerlink" href="#gtu" title="Permanent link">&para;</a></h3>
<p>GTU参考了GLU的思路，唯一的不同是在其中一个分支上使用了<code>Tno</code>，并且增加一个激活函数，写成数学公式即为：
<script type="math/tex; mode=display">
\mathbf O = [f({\mathbf X} {\mathbf W_1}) \odot (\mathbf T f({\mathbf X} {\mathbf W_2}))] {\mathbf W_3}.
</script>
实现如下：</p>
<pre><code class="language-python">class Gtu(nn.Module):
    def __init__(
        self,
        embed_dim,
        num_heads,
        bias=True,
        act_fun=&quot;silu&quot;,
        causal=False,
        expand_ratio=3,
        use_norm=False,
        norm_type=&quot;layernorm&quot;,
        use_decay=False,
        rpe_layers=3,
        rpe_embedding=512,
        rpe_act=&quot;relu&quot;,
        normalize=False,
        par_type=1,
        residual=False,
        gamma=0.99,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.expand_ratio = expand_ratio
        self.num_heads = num_heads
        self.normalize = normalize

        d1 = int(self.expand_ratio * embed_dim)
        d1 = (d1 // self.num_heads) * self.num_heads
        self.head_dim = d1 // num_heads
        # linear projection
        self.v_proj = nn.Linear(embed_dim, d1, bias=bias)
        self.u_proj = nn.Linear(embed_dim, d1, bias=bias)
        self.o = nn.Linear(d1, embed_dim, bias=bias)
        self.act = get_activation_fn(act_fun)
        # tno
        self.toep = Tno(
            h=num_heads, 
            dim=self.head_dim,
            rpe_dim=rpe_embedding, 
            causal=causal, 
            use_decay=use_decay, 
            residual=residual,
            act=rpe_act,
            par_type=par_type,
            gamma=gamma,
            bias=bias,
            layers=rpe_layers,
        )
        # norm
        self.norm_type = norm_type
        self.use_norm = use_norm

    def forward(self, x):
        # x: b, n, d
        num_heads = self.num_heads

        u = self.act(self.u_proj(x))
        v = self.act(self.v_proj(x))
        # reshape
        v = rearrange(v, 'b n (h d) -&gt; b h n d', h=num_heads)
        output = self.toep(v, dim=-2, normalize=self.normalize)
        output = rearrange(output, 'b h n d -&gt; b n (h d)')
        output = u * output
        output = self.o(output)

        return output
</code></pre>
<h3 id="tnnlayer">TnnLayer<a class="headerlink" href="#tnnlayer" title="Permanent link">&para;</a></h3>
<p>有了之前的准备工作，我们很容易实现出TnnLayer，因为这只不过是GTU和GLU的堆叠：</p>
<pre><code class="language-python">class TnnLayer(nn.Module):
    def __init__(
        self, 
        dim, 
        num_heads,
        rpe_embedding,
        glu_dim,
        # model params
        prenorm=True,
        norm_type=&quot;layernorm&quot;,
        # gtu params
        causal=False,
        gtu_act=&quot;silu&quot;,
        expand_ratio=3,
        use_decay=False,
        gamma=0.999,
        # rpe params
        rpe_act=&quot;relu&quot;,
        rpe_layers=3,
        # glu params
        glu_act=&quot;silu&quot;,
    ):
        super().__init__()
        self.token_mixer = Gtu(
            # gtu params
            embed_dim=dim,
            num_heads=num_heads,
            act_fun=gtu_act,
            norm_type=norm_type,
            causal=causal,
            expand_ratio=expand_ratio,
            use_decay=use_decay,
            gamma=gamma,
            # rpe params
            rpe_embedding=rpe_embedding,
            rpe_act=rpe_act,
            rpe_layers=rpe_layers,
        )

        self.token_norm = nn.LayerNorm(dim)
        self.feature_norm = nn.LayerNorm(dim)

        self.feature_mixer = GLU(
            d1=dim, 
            d2=glu_dim,
            act_fun=glu_act,
        )

    def forward(self, x):
        x = x + self.token_mixer(self.token_norm(x))
        x = x + self.feature_mixer(self.feature_norm(x))

        return x

</code></pre>
<p>在使用时，您只需要将TransformerLayer替换成TnnLayer即可。</p>
<h3 id="_15">小结<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h3>
<p>在本节中，我们完成了TnnLayer的实现，有了之前的铺垫工作，这一切并不困难。现在，您已经可以将Tnn应用到您的项目中了。</p>
<h1 id="_16">全文总结<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h1>
<p>通过之前的内容，您应该对TNN有所了解，这里，让我们对全文的核心进行总结：</p>
<ul>
<li>Transformer可以分为Token mixing和Channel mixing；</li>
<li>Attention的作用是Token mixing，而相对位置信息对Attention很重要，我们提出使用相对位置信息(Toepltiz matrix)来代替Attention Matrix；</li>
<li>使用Toeplitz matrix进行矩阵乘法可以加速，所以我们的方法理论上速度很快；</li>
<li>Toeplitz matrix的系数可以使用Rpe进行参数化，从而减少参数，结合指数衰减可以得到外推性；</li>
</ul>
<p>当然，TNN还有很多问题存在，例如：</p>
<ul>
<li>为什么相对位置信息就足够进行序列建模？</li>
<li>TNN真的只使用了相对位置信息吗？</li>
<li>TNN能达到理论速度上界吗？</li>
<li>TNN不能做哪些任务？</li>
<li>TNN有哪些先验假设？</li>
</ul>
<p>关于这些问题，我们将在后续的博客中回答，期待您的再次阅读。</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.b4d07000.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>